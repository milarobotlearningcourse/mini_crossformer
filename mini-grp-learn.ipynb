{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/robot_learning_2025.git\n",
    "%cd robot_learning_2025/hw2\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "# Install repo\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import rlds, numpy as np\n",
    "import mediapy as media\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install SimpleEnv for evaluation in the simulation\n",
    "# @title Install vulkan for rendering\n",
    "!apt-get install -yqq --no-install-recommends libvulkan-dev vulkan-tools\n",
    "# below fixes some bugs introduced by some recent Colab changes\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q -P /usr/share/vulkan/icd.d https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q -O /usr/share/glvnd/egl_vendor.d/10_nvidia.json https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Make sure vulkan is installed correctly\n",
    "!vulkaninfo | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/SimplerEnv.git --recurse-submodules\n",
    "!pip install -e ./SimplerEnv/ManiSkill2_real2sim/\n",
    "!pip install -e ./SimplerEnv\n",
    "!mkdir ./SimplerEnv/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "initialize(config_path=\"./conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import cv2\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch_grp(split, dataset, batch_size):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = dataset['train'] if split == 'train' else dataset['test']\n",
    "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
    "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
    "    x_goal = torch.tensor(data[\"goal\"][ix], dtype=torch.long)\n",
    "    x_goal_img = torch.tensor(data[\"goal_img\"][ix], dtype=torch.float)\n",
    "    y = torch.tensor(data[\"action\"][ix], dtype=torch.float)\n",
    "    return x, x_goal, x_goal_img, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(model._cfg.eval_iters)\n",
    "        for k in range(model._cfg.eval_iters):\n",
    "            X, x_goal, x_goal_img, Y = get_batch_grp(split, model._dataset, model._cfg.batch_size)\n",
    "            logits, loss = model(X, x_goal, x_goal_img, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // 8 ## n_patches = 8\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size)\n",
    "    return patches\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "## This is an encoder head (full attention)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        # [TODO]\n",
    "        \"\"\"\n",
    "        [DEFAULT]\n",
    "        # TODO: \n",
    "        ## Provide the block masking\n",
    "        pass\n",
    "        [/DEFAULT]\n",
    "        \"\"\"\n",
    "        if mask == None:\n",
    "            mask = torch.ones((T, ), device=self.device) ## (1, T)\n",
    "        # [/TODO]\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ### Block masked attention\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x,)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    self._dataset = dataset\n",
    "    self._cfg = cfg\n",
    "    # [TODO]\n",
    "    \"\"\"\n",
    "    [DEFAULT]\n",
    "    # TODO: \n",
    "    ## Provide the logic for the GRP network\n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    \n",
    "    [/DEFAULT]\n",
    "    \"\"\"\n",
    "    self.patch_size = (self._cfg.image_shape[0] / self._cfg.n_patches, self._cfg.image_shape[1] / self._cfg.n_patches)\n",
    "    #Positional embedding\n",
    "    self.register_buffer('positional_embeddings', calc_positional_embeddings(1 + self._cfg.n_patches ** 2 + self._cfg.block_size + self._cfg.n_patches ** 2, cfg.n_embd), persistent=False)\n",
    "\n",
    "    self.token_embedding_table = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
    "    self.class_tokens = nn.Parameter(torch.rand(1, cfg.n_embd))\n",
    "\n",
    "    self.input_d = int(self._cfg.image_shape[2] * self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "    self.lin_map = nn.Linear(self.input_d, self._cfg.n_embd, bias=False) \n",
    "\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([Block(self._cfg.n_embd, self._cfg.n_head, dropout=self._cfg.dropout) for _ in range(self._cfg.n_blocks)])\n",
    "\n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self._cfg.n_embd, self._cfg.action_bins),\n",
    "    )\n",
    "    # [/TODO]\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "      if isinstance(module, nn.Linear):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "          if module.bias is not None:\n",
    "              torch.nn.init.zeros_(module.bias)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, images, goals_txt, goal_imgs, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals_txt.shape\n",
    "    # [TODO]\n",
    "    \"\"\"\n",
    "    [DEFAULT]\n",
    "    # TODO: \n",
    "    ## Provide the logic to produce the output and loss for the GRP\n",
    "    \n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "\n",
    "    # Adding classification and goal_img tokens to the tokens\n",
    "\n",
    "    # Adding positional embedding\n",
    "\n",
    "    # Compute blocked masks\n",
    "\n",
    "    # Transformer Blocks\n",
    "\n",
    "    # Getting the classification token only\n",
    "\n",
    "    # Compute output and loss\n",
    "\n",
    "    [/DEFAULT]\n",
    "    \"\"\"\n",
    "    patches = get_patches_fast(images)\n",
    "    patches_g = get_patches_fast(goal_imgs)\n",
    "    goals_e = self.token_embedding_table(goals_txt)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    out = self.lin_map(patches)\n",
    "    out_g = self.lin_map(patches_g)\n",
    "    \n",
    "    # Adding classification and goal_img tokens to the tokens\n",
    "    out = torch.cat((self.class_tokens.expand(n, 1, -1), out, goals_e, out_g), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = out + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "    ## Compute blocked masks\n",
    "    mask = torch.ones((1 + c + T + c, ), device=self._cfg.device) ## (1, T)\n",
    "    if targets is None:\n",
    "        pass\n",
    "    elif (torch.rand(1)[0] > 0.66):  \n",
    "        mask[1 + c: 1 + c+ T] = torch.zeros((1,T), device=self._cfg.device) ## Mask goal string\n",
    "    elif (torch.rand(1)[0] > 0.33):\n",
    "        mask[1 + c + T: 1 + c + T + c] = torch.zeros((1,c), device=self._cfg.device) ## Mask goal image\n",
    "        \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out, mask)\n",
    "\n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    out = self.mlp(out)\n",
    "        \n",
    "    if targets is None:\n",
    "        loss = None\n",
    "    else:\n",
    "        B, C = out.shape\n",
    "        loss = F.mse_loss(out, targets) ## B, C\n",
    "    return (out, loss)\n",
    "    # [/TODO]\n",
    "\n",
    "import hydra, json\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import compose, initialize\n",
    "\n",
    "# @hydra.main(config_path=\"conf\", config_name=\"grp-mini\")\n",
    "# @hydra.main(config_path=\"./conf\", config_name=\"bridge-64-light\")\n",
    "def my_main():\n",
    "    cfg = compose(config_path=\"./conf\", config_name=\"bridge-64-light\", overrides=[\"+env=absolute_path\"])\n",
    "\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    log_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    cfg.device = device\n",
    "    from datasets import load_dataset, load_from_disk\n",
    "\n",
    "    dataset = load_dataset(cfg.dataset.to_name, split='train')\n",
    "    print('Features:', dataset.features)\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": np.array(dataset[\"img\"]),\n",
    "        \"action\": np.concatenate((np.array(dataset[\"action\"]) \n",
    "                                ,np.array(dataset[\"rotation_delta\"])\n",
    "                                ,np.array(dataset[\"open_gripper\"])\n",
    "                                ), axis=1),\n",
    "        \"goal_img\": np.array(dataset[\"goal_img\"]),\n",
    "        \"goal\": dataset[\"goal\"]\n",
    "    }\n",
    "    shortest_text_len = min([len(txt) for txt in dataset[\"goal\"]])\n",
    "    cfg.block_size = shortest_text_len\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set([item for row in dataset_tmp[\"goal\"] for item in row]))) ## Flatten to a long string\n",
    "    cfg.vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode_txt = lambda s: [stoi[c] for c in s] # text encoder to tokens: \n",
    "    decode_txy = lambda l: ''.join([itos[i] for i in l]) # token decoder to text: \n",
    "    print(\"vocab_size:\", cfg.vocab_size)\n",
    "    print(\"example text encode:\", encode_txt(dataset_tmp[\"goal\"][0]))\n",
    "\n",
    "    # [TODO]\n",
    "    \"\"\"\n",
    "    [DEFAULT]\n",
    "    # TODO: \n",
    "    ## Provide the logic for the GRP policy for discretized or continuous actions\n",
    "    \n",
    "    [/DEFAULT]\n",
    "    \"\"\"\n",
    "    if cfg.load_action_bounds == True:\n",
    "        a_std, a_mean = cfg.env.action_std, cfg.env.action_mean\n",
    "        a_std[6] = cfg.env.gripper_closed_std\n",
    "    else:\n",
    "        a_std, a_mean = (dataset_tmp[\"action\"].std(axis=0) + 0.001) * 1.5, dataset_tmp[\"action\"].mean(axis=0)\n",
    "    cfg.action_bins = len(a_mean)\n",
    "    encode_action = lambda af:   (((af - a_mean)/(a_std))).astype(np.float32) # encoder: take a float, output an integer\n",
    "    decode_action = lambda binN: (binN * a_std) + a_mean  # Undo mapping to [-1, 1]\n",
    "    # [/TODO]\n",
    "\n",
    "    ## Get the actions and encode them to map to [-1, 1]\n",
    "    encode_state = lambda af:   ((af/(255.0)*2.0)-1.0).astype(np.float32) # encoder: take a float, output an integer\n",
    "    resize_state = lambda sf:   cv2.resize(np.array(sf, dtype=np.float32), (cfg.image_shape[0], cfg.image_shape[1]))  # resize state\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": torch.tensor(encode_state(dataset_tmp[\"img\"])).to(device),\n",
    "        \"action\": torch.tensor(encode_action(dataset_tmp[\"action\"]), dtype=torch.float).to(device),            \n",
    "        \"goal_img\": torch.tensor(encode_state(dataset_tmp[\"goal_img\"])).to(device),\n",
    "        \"goal\": torch.tensor([encode_txt(goal[:cfg.block_size]) for goal in dataset_tmp[\"goal\"]]).to(device)\n",
    "    }\n",
    "\n",
    "    print(\"Dataset shape:\", len(dataset_tmp[\"img\"]))\n",
    "    dataset_tmp = {\"train\": dataset_tmp, \"test\": dataset_tmp} \n",
    "    if not cfg.testing:\n",
    "        import wandb\n",
    "        # start a new wandb run to track this script\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.experiment.project,\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config= OmegaConf.to_container(cfg)\n",
    "        )\n",
    "        wandb.run.log_code(\".\")\n",
    "    model = GRP(dataset_tmp, cfg)\n",
    "    m = model.to(device)\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    import torch.optim.lr_scheduler as lr_scheduler\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=cfg.max_iters)\n",
    "\n",
    "    if cfg.simEval:\n",
    "        import simpler_env\n",
    "        from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
    "        task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
    "        if 'env' in locals():\n",
    "            print(\"Closing existing env\")\n",
    "            env.close()\n",
    "            del env\n",
    "        env = simpler_env.make(task_name)\n",
    "        env_unwrapped = env.env.env.env ## Updated gymnasium wrapper adds lots of wrappers.\n",
    "\n",
    "    for iter in range(cfg.max_iters):\n",
    "\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if not cfg.testing:\n",
    "                wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "            if cfg.simEval and (iter % cfg.eval_vid_iters == 0): ## Do this eval infrequently because it takes a fiar bit of compute\n",
    "                rewards = []\n",
    "                for j in range(cfg.sim.eval_episodes): ## Better to eval over a few different goal configurations\n",
    "                    obs, reset_info = env.reset()\n",
    "                    instruction = env_unwrapped.get_language_instruction()\n",
    "                    print(\"Reset info\", reset_info)\n",
    "                    print(\"Instruction\", instruction)\n",
    "                    frames = []\n",
    "                    done, truncated, timeLimit, t = False, False, 100, 0\n",
    "                    while not (done or truncated or (t > timeLimit)):\n",
    "                        # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
    "                        # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
    "                        image = get_image_from_maniskill2_obs_dict(env_unwrapped, obs)\n",
    "                        image = image[:,:,:3] ## Remove last dimension of image color\n",
    "                        action, loss = model.forward(torch.tensor(np.array([encode_state(resize_state(image))])).to(device)\n",
    "                                            ,torch.tensor(np.array([encode_txt(instruction)[:cfg.block_size]])).to(device) ## There can be issues here if th text is shorter than any example in the dataset\n",
    "                                            ,torch.tensor(np.array([encode_state(resize_state(image))])).to(device) ## Not the correct goal image... Should mask this.\n",
    "                                            )\n",
    "                        # action = env.action_space.sample() # replace this with your policy inference\n",
    "                        if cfg.load_action_bounds:\n",
    "                            action = decode_action(action.cpu().detach().numpy()[0]) ## Add in the gripper close action\n",
    "                        else:\n",
    "                            action = np.concatenate((decode_action(action.cpu().detach().numpy()[0]), [0]), axis = -1) ## Add in the gripper close action\n",
    "                        obs, reward, done, truncated, info = env.step(action)\n",
    "                        reward = -np.linalg.norm(info[\"eof_to_obj1_diff\"])\n",
    "                        frames.append(image)\n",
    "                        rewards.append(reward)\n",
    "                        t=t+1\n",
    "                \n",
    "                episode_stats = info.get('episode_stats', {})\n",
    "                print(\"Episode stats\", episode_stats)\n",
    "                print(f\"avg reward {np.mean(rewards):.8f}\")\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"avg reward\": np.mean(rewards)})\n",
    "                import moviepy.editor as mpy\n",
    "                clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
    "                clip.write_videofile(log_dir+\"/sim-env-\"+str(iter)+\".mp4\", fps=20)\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"example\": wandb.Video(log_dir+\"/sim-env-\"+str(iter)+\".mp4\")})\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, xg, xgi, yb = get_batch_grp('train', dataset_tmp, cfg.batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, xg, xgi, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        if (iter + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if not cfg.testing:\n",
    "        wandb.finish()\n",
    "    return losses['val']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = my_main()\n",
    "    print(\"results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-grp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
