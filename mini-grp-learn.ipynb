{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/robot_learning_2025.git\n",
    "%cd robot_learning_2025/hw2\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "# Install repo\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import rlds, numpy as np\n",
    "import mediapy as media\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "def as_gif(images, path='temp.gif'):\n",
    "  # Render the images as the gif:\n",
    "  images = [Image.fromarray(image) for image in images]\n",
    "  images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n",
    "  gif_bytes = open(path,'rb').read()\n",
    "  return gif_bytes\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "dataset = builder.as_dataset(split='train[:1]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(dataset))\n",
    "print(episode)\n",
    "\n",
    "steps = list(episode['steps'])\n",
    "images = np.array([cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps])\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "display.Image(as_gif(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the actions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actions = np.array([np.array(step['action']['world_vector']) for step in steps])\n",
    "\n",
    "plt.plot(actions[:, 0], label='x')\n",
    "plt.plot(actions[:, 1], label='y')\n",
    "plt.plot(actions[:, 2], label='z')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install SimpleEnv for evaluation in the simulation\n",
    "# @title Install vulkan for rendering\n",
    "!apt-get install -yqq --no-install-recommends libvulkan-dev vulkan-tools\n",
    "# below fixes some bugs introduced by some recent Colab changes\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q -P /usr/share/vulkan/icd.d https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q -O /usr/share/glvnd/egl_vendor.d/10_nvidia.json https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Make sure vulkan is installed correctly\n",
    "!vulkaninfo | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/SimplerEnv.git --recurse-submodules\n",
    "!pip install -e ./SimplerEnv/ManiSkill2_real2sim/\n",
    "!pip install -e ./SimplerEnv\n",
    "!mkdir ./SimplerEnv/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpler_env\n",
    "from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
    "import mediapy\n",
    "import sapien.core as sapien\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
    "\n",
    "if 'env' in locals():\n",
    "  print(\"Closing existing env\")\n",
    "  env.close()\n",
    "  del env\n",
    "env = simpler_env.make(task_name)\n",
    "# Colab GPU does not support denoiser\n",
    "sapien.render_config.rt_use_denoiser = False\n",
    "obs, reset_info = env.reset()\n",
    "instruction = env.get_language_instruction()\n",
    "print(\"Reset info\", reset_info)\n",
    "print(\"Instruction\", instruction)\n",
    "\n",
    "frames = []\n",
    "done, truncated = False, False\n",
    "while not (done or truncated):\n",
    "   # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
    "   # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
    "   image = get_image_from_maniskill2_obs_dict(env, obs)\n",
    "   action = env.action_space.sample() # replace this with your policy inference\n",
    "   obs, reward, done, truncated, info = env.step(action)\n",
    "   frames.append(image)\n",
    "\n",
    "episode_stats = info.get('episode_stats', {})\n",
    "print(\"Episode stats\", episode_stats)\n",
    "# mediapy.show_video(frames, fps=10)\n",
    "import moviepy.editor as mpy\n",
    "clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
    "display.Image(as_gif(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "initialize(config_path=\"./conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import cv2\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch_grp(split, dataset, batch_size):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = dataset['train'] if split == 'train' else dataset['test']\n",
    "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
    "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
    "    x_goal = torch.tensor(data[\"goal\"][ix], dtype=torch.long)\n",
    "    x_goal_img = torch.tensor(data[\"goal_img\"][ix], dtype=torch.float)\n",
    "    y = torch.tensor(data[\"action\"][ix], dtype=torch.float)\n",
    "    return x, x_goal, x_goal_img, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(model._cfg.eval_iters)\n",
    "        for k in range(model._cfg.eval_iters):\n",
    "            X, x_goal, x_goal_img, Y = get_batch_grp(split, model._dataset, model._cfg.batch_size)\n",
    "            logits, loss = model(X, x_goal, x_goal_img, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // 8 ## n_patches = 8\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size)\n",
    "    return patches\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "## This is an encoder head (full attention)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        TODO\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ### Block masked attention\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x,)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    self._dataset = dataset\n",
    "    self._cfg = cfg\n",
    "    TODO\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "      if isinstance(module, nn.Linear):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "          if module.bias is not None:\n",
    "              torch.nn.init.zeros_(module.bias)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, images, goals_txt, goal_imgs, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals_txt.shape\n",
    "    TODO\n",
    "\n",
    "import hydra, json\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import compose, initialize\n",
    "\n",
    "# @hydra.main(config_path=\"conf\", config_name=\"grp-mini\")\n",
    "# @hydra.main(config_path=\"./conf\", config_name=\"bridge-64-light\")\n",
    "def my_main():\n",
    "    cfg = compose(config_path=\"./conf\", config_name=\"bridge-64-light\", overrides=[\"+env=absolute_path\"])\n",
    "\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    log_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    cfg.device = device\n",
    "    from datasets import load_dataset, load_from_disk\n",
    "\n",
    "    dataset = load_dataset(cfg.dataset.to_name, split='train')\n",
    "    print('Features:', dataset.features)\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": np.array(dataset[\"img\"]),\n",
    "        \"action\": np.concatenate((np.array(dataset[\"action\"]) \n",
    "                                ,np.array(dataset[\"rotation_delta\"])\n",
    "                                ,np.array(dataset[\"open_gripper\"])\n",
    "                                ), axis=1),\n",
    "        \"goal_img\": np.array(dataset[\"goal_img\"]),\n",
    "        \"goal\": dataset[\"goal\"]\n",
    "    }\n",
    "    shortest_text_len = min([len(txt) for txt in dataset[\"goal\"]])\n",
    "    cfg.block_size = shortest_text_len\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set([item for row in dataset_tmp[\"goal\"] for item in row]))) ## Flatten to a long string\n",
    "    cfg.vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode_txt = lambda s: [stoi[c] for c in s] # text encoder to tokens: \n",
    "    decode_txy = lambda l: ''.join([itos[i] for i in l]) # token decoder to text: \n",
    "    print(\"vocab_size:\", cfg.vocab_size)\n",
    "    print(\"example text encode:\", encode_txt(dataset_tmp[\"goal\"][0]))\n",
    "\n",
    "    TODO\n",
    "\n",
    "    ## Get the actions and encode them to map to [-1, 1]\n",
    "    encode_state = lambda af:   ((af/(255.0)*2.0)-1.0).astype(np.float32) # encoder: take a float, output an integer\n",
    "    resize_state = lambda sf:   cv2.resize(np.array(sf, dtype=np.float32), (cfg.image_shape[0], cfg.image_shape[1]))  # resize state\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": torch.tensor(encode_state(dataset_tmp[\"img\"])).to(device),\n",
    "        \"action\": torch.tensor(encode_action(dataset_tmp[\"action\"]), dtype=torch.float).to(device),            \n",
    "        \"goal_img\": torch.tensor(encode_state(dataset_tmp[\"goal_img\"])).to(device),\n",
    "        \"goal\": torch.tensor([encode_txt(goal[:cfg.block_size]) for goal in dataset_tmp[\"goal\"]]).to(device)\n",
    "    }\n",
    "\n",
    "    print(\"Dataset shape:\", len(dataset_tmp[\"img\"]))\n",
    "    dataset_tmp = {\"train\": dataset_tmp, \"test\": dataset_tmp} \n",
    "    if not cfg.testing:\n",
    "        import wandb\n",
    "        # start a new wandb run to track this script\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.experiment.project,\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config= OmegaConf.to_container(cfg)\n",
    "        )\n",
    "        wandb.run.log_code(\".\")\n",
    "    model = GRP(dataset_tmp, cfg)\n",
    "    m = model.to(device)\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    import torch.optim.lr_scheduler as lr_scheduler\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=cfg.max_iters)\n",
    "\n",
    "    if cfg.simEval:\n",
    "        import simpler_env\n",
    "        from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
    "        task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
    "        if 'env' in locals():\n",
    "            print(\"Closing existing env\")\n",
    "            env.close()\n",
    "            del env\n",
    "        env = simpler_env.make(task_name)\n",
    "        env_unwrapped = env.env.env.env ## Updated gymnasium wrapper adds lots of wrappers.\n",
    "\n",
    "    for iter in range(cfg.max_iters):\n",
    "\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if not cfg.testing:\n",
    "                wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "            if cfg.simEval and (iter % cfg.eval_vid_iters == 0): ## Do this eval infrequently because it takes a fiar bit of compute\n",
    "                rewards = []\n",
    "                for j in range(cfg.sim.eval_episodes): ## Better to eval over a few different goal configurations\n",
    "                    obs, reset_info = env.reset()\n",
    "                    instruction = env_unwrapped.get_language_instruction()\n",
    "                    print(\"Reset info\", reset_info)\n",
    "                    print(\"Instruction\", instruction)\n",
    "                    frames = []\n",
    "                    done, truncated, timeLimit, t = False, False, 100, 0\n",
    "                    while not (done or truncated or (t > timeLimit)):\n",
    "                        # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
    "                        # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
    "                        image = get_image_from_maniskill2_obs_dict(env_unwrapped, obs)\n",
    "                        image = image[:,:,:3] ## Remove last dimension of image color\n",
    "                        action, loss = model.forward(torch.tensor(np.array([encode_state(resize_state(image))])).to(device)\n",
    "                                            ,torch.tensor(np.array([encode_txt(instruction)[:cfg.block_size]])).to(device) ## There can be issues here if th text is shorter than any example in the dataset\n",
    "                                            ,torch.tensor(np.array([encode_state(resize_state(image))])).to(device) ## Not the correct goal image... Should mask this.\n",
    "                                            )\n",
    "                        # action = env.action_space.sample() # replace this with your policy inference\n",
    "                        if cfg.load_action_bounds:\n",
    "                            action = decode_action(action.cpu().detach().numpy()[0]) ## Add in the gripper close action\n",
    "                        else:\n",
    "                            action = np.concatenate((decode_action(action.cpu().detach().numpy()[0]), [0]), axis = -1) ## Add in the gripper close action\n",
    "                        obs, reward, done, truncated, info = env.step(action)\n",
    "                        reward = -np.linalg.norm(info[\"eof_to_obj1_diff\"])\n",
    "                        frames.append(image)\n",
    "                        rewards.append(reward)\n",
    "                        t=t+1\n",
    "                \n",
    "                episode_stats = info.get('episode_stats', {})\n",
    "                print(\"Episode stats\", episode_stats)\n",
    "                print(f\"avg reward {np.mean(rewards):.8f}\")\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"avg reward\": np.mean(rewards)})\n",
    "                import moviepy.editor as mpy\n",
    "                clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
    "                clip.write_videofile(log_dir+\"/sim-env-\"+str(iter)+\".mp4\", fps=20)\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"example\": wandb.Video(log_dir+\"/sim-env-\"+str(iter)+\".mp4\")})\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, xg, xgi, yb = get_batch_grp('train', dataset_tmp, cfg.batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, xg, xgi, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        if (iter + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if not cfg.testing:\n",
    "        wandb.finish()\n",
    "    return losses['val']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = my_main()\n",
    "    print(\"results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-grp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
